{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laser Machine Listener\n",
    "\n",
    "The following code implements machine learning with Chainer to classify sounds emitted by a laser machine during processing such as cutting and marking. The assumed states are as follows. As learning data, prepare folders as many as the number of states in the **data** folder, and place one or more sound files corresponding to each state in each folder.\n",
    "\n",
    "- Background\n",
    "- Cutting (in focus)\n",
    "- Cutting (not in focus)\n",
    "- Marking\n",
    "- Waiting\n",
    "- Sleeping\n",
    "\n",
    "以下のコードは、レーザー加工機がカットやマーキングなどの加工中に発する音を分類するための機械学習をChainerで実装したものです。想定している状態は以下の通りです。学習用のデータとして、**data**フォルダの中に状態の数だけフォルダを用意し、それぞれのフォルダの中にそれぞれの状態に対応する1つまたはそれ以上の音声ファイルを配置してください。\n",
    "\n",
    "- 背景音\n",
    "- カット中（焦点が合っている）\n",
    "- カット中（焦点が合っていない）\n",
    "- マーキング中\n",
    "- 待機中\n",
    "- スリープ中\n",
    "\n",
    "| State | Laser Machine | Dust collector | Ventilator |\n",
    "| --: | :-- | :-- | :-- |\n",
    "| Background | OFF | OFF | OFF |\n",
    "| Sleeping | OFF | OFF | ON |\n",
    "| Waiting | ON | ON | ON |\n",
    "| Cutting (in focus) | ON | ON | ON |\n",
    "| Cutting (not in focus) | ON | ON | ON |\n",
    "| Marking | ON | ON | ON |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we initialise an array for containing state names and a matrix for storing voice data and correct labels. Next, read sound files matching the pattern of the path for 30 seconds in order, normalize, obtain STFT, and register together with the same number of correct answer labels.\n",
    "\n",
    "まず、状態名を収めるための配列と、音声データと正解ラベルを収めるための行列を初期化する。次に、パスのパターンに合致する音声ファイルを順に30秒間ずつ読み込み、正規化したのちにSTFTを求め、同数の正解ラベルと共に登録する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import librosa\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "FFT_SIZE = 256\n",
    "STFT_MATRIX_SIZE = 1 + FFT_SIZE // 2\n",
    "\n",
    "state_names = []\n",
    "data = np.empty((0, STFT_MATRIX_SIZE), dtype=np.float32)\n",
    "index = np.empty(0, dtype=np.int32)\n",
    "\n",
    "for path_name in sorted(glob.glob('data/*/*.wav')):\n",
    "    state_name = path_name.split('/')[1]\n",
    "\n",
    "    if state_name not in state_names:\n",
    "        state_names.append(state_name)\n",
    "\n",
    "    audio, sr = librosa.load(path_name, sr=SAMPLING_RATE, duration=30)\n",
    "    print('{}: {} ({} Hz) '.format(state_name, path_name, sr))\n",
    "    d = np.abs(librosa.stft(librosa.util.normalize(audio),\n",
    "                            n_fft=FFT_SIZE, window='hamming'))\n",
    "    data = np.vstack((data, d.transpose()))\n",
    "    index = np.hstack([index, [state_names.index(state_name)] * d.shape[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check what kind of difference is seen when viewing each state on the frequency axis, we calculate the average of STFT for each state and draw with matplotlib.\n",
    "\n",
    "それぞれの状態を周波数軸で見たときにどのような違いがあるのかを確認するため、状態ごとにSTFTの平均を求め、matplotlibを用いて描画します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n_states = len(state_names)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5 * n_states))\n",
    "\n",
    "for i, state_name in enumerate(state_names):\n",
    "    plt.subplot(n_states, 1, 1 + i)\n",
    "    plt.plot(librosa.fft_frequencies(sr=SAMPLING_RATE, n_fft=FFT_SIZE),\n",
    "             np.mean(data[np.where(index == i)], axis=0))\n",
    "    plt.title(state_name)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that data is ready, we define a neural network with Chainer, generate a model, and prepare a dataset with data and correct index as a set. Of the data set, we will use 80% for learning and the remaining 20% for verification.\n",
    "\n",
    "以上でデータが用意できましたので、Chainerでニューラルネットワークを定義してモデルを生成し、データと正解インデックスをセットにしたデータセットを用意します。データセットのうち、8割を学習用、残りの2割を検証用に用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "\n",
    "\n",
    "class LaserMachineListener(chainer.Chain):\n",
    "\n",
    "    def __init__(self, n_mid_units, n_out):\n",
    "        super(LaserMachineListener, self).__init__()\n",
    "\n",
    "        with self.init_scope():\n",
    "            self.l1 = L.Linear(None, n_mid_units)\n",
    "            self.l2 = L.Linear(None, n_mid_units // 2)\n",
    "            self.l3 = L.Linear(None, n_out)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        h1 = F.relu(self.l1(x))\n",
    "        h2 = F.relu(self.l2(h1))\n",
    "        return self.l3(h2)\n",
    "\n",
    "\n",
    "net = LaserMachineListener(n_mid_units=128, n_out=n_states)\n",
    "model = L.Classifier(net)\n",
    "\n",
    "dataset = chainer.datasets.TupleDataset(data, index)\n",
    "n_train = int(len(dataset) * 0.8)\n",
    "train, test = chainer.datasets.split_dataset_random(dataset, n_train, seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select Adam as the optimisation method set the number of epochs to 10, configure to report progress in progress during learning, and execute learning. When using files bundled as samples, the correct answer rate is about 95%. If you do not get the proper answer rate for the samples prepared by yourself, try tuning parameters such as `n_mid_units` and `BATCH_SIZE`.\n",
    "\n",
    "最適化手法としてAdamを選択し、エポック数は10に設定し、学習中の途中経過をレポートするように設定した上で学習を実行します。サンプルとして同梱しているファイルを用いた場合、正解率は約95%程度になります。もし、自分で用意したサンプルに対する正解率が上がらない場合には、`n_mid_units`や`BATCH_SIZE`などのパラメータをチューニングしてみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from chainer import optimizers\n",
    "from chainer import training\n",
    "\n",
    "optimizer = optimizers.Adam(alpha=0.01)\n",
    "optimizer.setup(model)\n",
    "\n",
    "GPU_ID = -1\n",
    "\n",
    "# Specify the seed value of the random number\n",
    "# so that the same result will be obtained each time\n",
    "np.random.seed(1)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iter = chainer.iterators.SerialIterator(train, BATCH_SIZE)\n",
    "test_iter = chainer.iterators.SerialIterator(test, BATCH_SIZE,\n",
    "                                             repeat=False, shuffle=False)\n",
    "updater = training.StandardUpdater(train_iter, optimizer, device=GPU_ID)\n",
    "\n",
    "MAX_EPOCH = 20\n",
    "\n",
    "trainer = training.Trainer(updater, (MAX_EPOCH, 'epoch'), out='result')\n",
    "\n",
    "from chainer.training import extensions\n",
    "\n",
    "trainer.extend(extensions.LogReport())\n",
    "trainer.extend(extensions.Evaluator(\n",
    "    test_iter, model, device=GPU_ID), name='val')\n",
    "trainer.extend(extensions.PrintReport(\n",
    "    ['epoch',\n",
    "     'main/loss', 'main/accuracy', 'val/main/loss', 'val/main/accuracy',\n",
    "     'elapsed_time']))\n",
    "trainer.extend(extensions.PlotReport(\n",
    "    ['main/loss', 'val/main/loss'],\n",
    "    x_key='epoch', file_name='loss.png'))\n",
    "trainer.extend(extensions.PlotReport(\n",
    "    ['main/accuracy', 'val/main/accuracy'],\n",
    "    x_key='epoch', file_name='accuracy.png'))\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When learning is over, save the array containing the state name and the learned model as separate files.\n",
    "\n",
    "学習が終わったら、状態の名前を収めた配列と学習済みのモデルを個別のファイルとして保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('state_names.json', 'w') as file_to_save:\n",
    "    json.dump(state_names, file_to_save)\n",
    "\n",
    "model.to_cpu()\n",
    "chainer.serializers.save_npz('laser_machine_listener.model', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
